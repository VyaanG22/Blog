{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unit 2.3 Extracting Information from Data, Pandas\n",
    "> Data connections, trends, and correlation.  Pandas is introduced as it could be valuable for PBL, data validation, as well as understanding College Board Topics.\n",
    "- toc: true\n",
    "- image: /images/python.png\n",
    "- categories: []\n",
    "- type: ap\n",
    "- week: 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files To Get\n",
    "\n",
    "Save this file to your **_notebooks** folder\n",
    "\n",
    "wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb\n",
    "\n",
    "Save these files into a subfolder named **files** in your **_notebooks** folder\n",
    "\n",
    "wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv\n",
    "\n",
    "wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json\n",
    "\n",
    "Save this image into a subfolder named **images** in your **_notebooks** folder\n",
    "\n",
    "wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and DataFrames\n",
    "> In this lesson we will be exploring data analysis using Pandas.  \n",
    "\n",
    "- College Board talks about ideas like \n",
    "    - Tools. \"the ability to process data depends on users capabilities and their tools\"\n",
    "    - Combining Data.  \"combine county data sets\"\n",
    "    - Status on Data\"determining the artist with the greatest attendance during a particular month\"\n",
    "    - Data poses challenge. \"the need to clean data\", \"incomplete data\"\n",
    "\n",
    "\n",
    "- [From Pandas Overview](https://pandas.pydata.org/docs/getting_started/index.html) -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame.\n",
    "\n",
    "\n",
    "![DataFrame](images/table_dataframe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pandas is used to gather data sets through its DataFrames implementation'''\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "\n",
    "When looking at a data set, check to see what data needs to be cleaned. Examples include:\n",
    "- Missing Data Points\n",
    "- Invalid Data\n",
    "- Inaccurate Data\n",
    "\n",
    "Run the following code to see what needs to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Student ID Year in School   GPA\n",
      "0         123             12  3.57\n",
      "1         246             10  4.00\n",
      "2         578             12  2.78\n",
      "3         469             11  3.45\n",
      "4         324         Junior  4.75\n",
      "5         313             20  3.33\n",
      "6         145             12  2.95\n",
      "7         167             10  3.90\n",
      "8         235      9th Grade  3.15\n",
      "9         nil              9  2.80\n",
      "10        469             11  3.45\n",
      "11        456             10  2.75\n"
     ]
    }
   ],
   "source": [
    "# reads the JSON file and converts it to a Pandas DataFrame\n",
    "df = pd.read_json('files/grade.json')\n",
    "\n",
    "print(df)\n",
    "# What part of the data set needs to be cleaned?\n",
    "# From PBL learning, what is a good time to clean data?  Hint, remember Garbage in, Garbage out?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Info\n",
    "\n",
    "Take a look at some features that the Pandas library has that extracts info from the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Extract Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     GPA\n",
      "0   3.57\n",
      "1   4.00\n",
      "2   2.78\n",
      "3   3.45\n",
      "4   4.75\n",
      "5   3.33\n",
      "6   2.95\n",
      "7   3.90\n",
      "8   3.15\n",
      "9   2.80\n",
      "10  3.45\n",
      "11  2.75\n",
      "\n",
      "Student ID  GPA\n",
      "       123 3.57\n",
      "       246 4.00\n",
      "       578 2.78\n",
      "       469 3.45\n",
      "       324 4.75\n",
      "       313 3.33\n",
      "       145 2.95\n",
      "       167 3.90\n",
      "       235 3.15\n",
      "       nil 2.80\n",
      "       469 3.45\n",
      "       456 2.75\n"
     ]
    }
   ],
   "source": [
    "#print the values in the points column with column header\n",
    "print(df[['GPA']])\n",
    "\n",
    "print()\n",
    "\n",
    "#try two columns and remove the index from print statement\n",
    "print(df[['Student ID','GPA']].to_string(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Student ID Year in School   GPA\n",
      "11        456             10  2.75\n",
      "2         578             12  2.78\n",
      "9         nil              9  2.80\n",
      "6         145             12  2.95\n",
      "8         235      9th Grade  3.15\n",
      "5         313             20  3.33\n",
      "3         469             11  3.45\n",
      "10        469             11  3.45\n",
      "0         123             12  3.57\n",
      "7         167             10  3.90\n",
      "1         246             10  4.00\n",
      "4         324         Junior  4.75\n",
      "\n",
      "   Student ID Year in School   GPA\n",
      "4         324         Junior  4.75\n",
      "1         246             10  4.00\n",
      "7         167             10  3.90\n",
      "0         123             12  3.57\n",
      "3         469             11  3.45\n",
      "10        469             11  3.45\n",
      "5         313             20  3.33\n",
      "8         235      9th Grade  3.15\n",
      "6         145             12  2.95\n",
      "9         nil              9  2.80\n",
      "2         578             12  2.78\n",
      "11        456             10  2.75\n"
     ]
    }
   ],
   "source": [
    "#sort values\n",
    "print(df.sort_values(by=['GPA']))\n",
    "\n",
    "print()\n",
    "\n",
    "#sort the values in reverse order\n",
    "print(df.sort_values(by=['GPA'], ascending=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Selection or Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Student ID Year in School   GPA\n",
      "0         123             12  3.57\n",
      "1         246             10  4.00\n",
      "3         469             11  3.45\n",
      "4         324         Junior  4.75\n",
      "5         313             20  3.33\n",
      "7         167             10  3.90\n",
      "8         235      9th Grade  3.15\n",
      "10        469             11  3.45\n"
     ]
    }
   ],
   "source": [
    "#print only values with a specific criteria \n",
    "print(df[df.GPA > 3.00])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Selection Max and Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Student ID Year in School   GPA\n",
      "4        324         Junior  4.75\n",
      "\n",
      "   Student ID Year in School   GPA\n",
      "11        456             10  2.75\n"
     ]
    }
   ],
   "source": [
    "print(df[df.GPA == df.GPA.max()])\n",
    "print()\n",
    "print(df[df.GPA == df.GPA.min()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own DataFrame\n",
    "\n",
    "Using Pandas allows you to create your own DataFrame in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Dictionary to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Dict_to_DF------------------\n",
      "   calories  duration\n",
      "0       420        50\n",
      "1       380        40\n",
      "2       390        45\n",
      "----------Dict_to_DF_labels--------------\n",
      "      calories  duration\n",
      "day1       420        50\n",
      "day2       380        40\n",
      "day3       390        45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#the data can be stored as a python dictionary\n",
    "dict = {\n",
    "  \"calories\": [420, 380, 390],\n",
    "  \"duration\": [50, 40, 45]\n",
    "}\n",
    "#stores the data in a data frame\n",
    "print(\"-------------Dict_to_DF------------------\")\n",
    "df = pd.DataFrame(dict)\n",
    "print(df)\n",
    "\n",
    "print(\"----------Dict_to_DF_labels--------------\")\n",
    "\n",
    "#or with the index argument, you can label rows.\n",
    "df = pd.DataFrame(dict, index = [\"day1\", \"day2\", \"day3\"])\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine DataFrame Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Examine Selected Rows---------\n",
      "      calories  duration\n",
      "day1       420        50\n",
      "day3       390        45\n",
      "--------Examine Single Row-----------\n",
      "calories    420\n",
      "duration     50\n",
      "Name: day1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"-------Examine Selected Rows---------\")\n",
    "#use a list for multiple labels:\n",
    "print(df.loc[[\"day1\", \"day3\"]])\n",
    "\n",
    "#refer to the row index:\n",
    "print(\"--------Examine Single Row-----------\")\n",
    "print(df.loc[\"day1\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DataFrame Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3 entries, day1 to day3\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   calories  3 non-null      int64\n",
      " 1   duration  3 non-null      int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 180.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print info about the data set\n",
    "print(df.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of larger data set\n",
    "\n",
    "Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Duration Top 10---------\n",
      "     Duration  Pulse  Maxpulse  Calories\n",
      "69        300    108       143    1500.2\n",
      "79        270    100       131    1729.0\n",
      "109       210    137       184    1860.4\n",
      "60        210    108       160    1376.0\n",
      "106       180     90       120     800.3\n",
      "90        180    101       127     600.1\n",
      "65        180     90       130     800.4\n",
      "61        160    110       137    1034.4\n",
      "62        160    109       135     853.0\n",
      "67        150    107       130     816.0\n",
      "--Duration Bottom 10------\n",
      "     Duration  Pulse  Maxpulse  Calories\n",
      "68         20    106       136     110.4\n",
      "100        20     95       112      77.7\n",
      "89         20     83       107      50.3\n",
      "135        20    136       156     189.0\n",
      "94         20    150       171     127.4\n",
      "95         20    151       168     229.4\n",
      "139        20    141       162     222.4\n",
      "64         20    110       130     131.4\n",
      "112        15    124       139     124.2\n",
      "93         15     80       100      50.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read csv and sort 'Duration' largest to smallest\n",
    "df = pd.read_csv('files/data.csv').sort_values(by=['Duration'], ascending=False)\n",
    "\n",
    "print(\"--Duration Top 10---------\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"--Duration Bottom 10------\")\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs are a Source for Writing Programs with Data\n",
    "> 3rd Party APIs are a great source for creating Pandas Data Frames.  \n",
    "- Data can be fetched and resulting json can be placed into a Data Frame\n",
    "- Observe output, this looks very similar to a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_name       cases     deaths\n",
      "0          USA  82,649,779  1,018,316\n",
      "1        India  43,057,545    522,193\n",
      "2       Brazil  30,345,654    662,663\n",
      "3       France  28,244,977    145,020\n",
      "4      Germany  24,109,433    134,624\n",
      "5           UK  21,933,206    173,352\n"
     ]
    }
   ],
   "source": [
    "'''Pandas can be used to analyze data'''\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def fetch():\n",
    "    '''Obtain data from an endpoint'''\n",
    "    url = \"https://flask.nighthawkcodingsociety.com/api/covid/\"\n",
    "    fetch = requests.get(url)\n",
    "    json = fetch.json()\n",
    "\n",
    "    # filter data for requirement\n",
    "    df = pd.DataFrame(json['countries_stat'])  # filter endpoint for country stats\n",
    "    print(df.loc[0:5, 'country_name':'deaths']) # show row 0 through 5 and columns country_name through deaths\n",
    "    \n",
    "fetch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacks\n",
    "> Early Seed award\n",
    "- Add this Blog to you own Blogging site.\n",
    "- Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don't tell anyone. Show to Teacher.\n",
    "\n",
    "> AP Prep\n",
    "- Add this Blog to you own Blogging site.  In the Blog add notes and observations on each code cell.\n",
    "- In blog add College Board practice problems for 2.3.\n",
    "\n",
    "> The next 4 weeks, Teachers want you to improve your understanding of data.  Look at the blog and others on Unit 2.  Your intention is to find some things to differentiate your individual College Board project.\n",
    "\n",
    "- Create or Find your own dataset.  The suggestion is to use a JSON file, integrating with your PBL project would be ***Amazing***.\n",
    "\n",
    "- When choosing a data set, think about the following...\n",
    "\n",
    "    - Does it have a good sample size?\n",
    "    - Is there bias in the data?\n",
    "    - Does the data set need to be cleaned?\n",
    "    - What is the purpose of the data set?\n",
    "    - ...\n",
    "\n",
    "- Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.)\n",
    "\n",
    "## Hack Helpers\n",
    "> Here is how Mort started on this assignment by asking ChatGPT  ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas?\n",
    "\n",
    "- There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. \n",
    "\n",
    "    Here are some suggestions...\n",
    "\n",
    "    - Titanic Dataset: This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status.\n",
    "\n",
    "    - Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "    - NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas.\n",
    "\n",
    "    - Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas.\n",
    "\n",
    "    - COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas.\n",
    "\n",
    "    - World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas.\n",
    "\n",
    "\n",
    "- These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop.\n",
    "\n",
    "\n",
    "> Follow up question, \"where can I find Titanic data set?\"\n",
    "\n",
    "- The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset...\n",
    "\n",
    "    - Kaggle: The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account.\n",
    "\n",
    "    - UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website.\n",
    "\n",
    "    - Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code:\n",
    "\n",
    "    ```python\n",
    "    import seaborn as sns\n",
    "    titanic_data = sns.load_dataset('titanic')\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Data\n",
    "> Look at a sample of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic Data\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n",
      "     survived  pclass     sex   age  sibsp  parch   class     fare  \\\n",
      "0           0       3    male  22.0      1      0   Third   7.2500   \n",
      "1           1       1  female  38.0      1      0   First  71.2833   \n",
      "2           1       3  female  26.0      0      0   Third   7.9250   \n",
      "3           1       1  female  35.0      1      0   First  53.1000   \n",
      "4           0       3    male  35.0      0      0   Third   8.0500   \n",
      "..        ...     ...     ...   ...    ...    ...     ...      ...   \n",
      "886         0       2    male  27.0      0      0  Second  13.0000   \n",
      "887         1       1  female  19.0      0      0   First  30.0000   \n",
      "888         0       3  female   NaN      1      2   Third  23.4500   \n",
      "889         1       1    male  26.0      0      0   First  30.0000   \n",
      "890         0       3    male  32.0      0      0   Third   7.7500   \n",
      "\n",
      "     embark_town  \n",
      "0    Southampton  \n",
      "1      Cherbourg  \n",
      "2    Southampton  \n",
      "3    Southampton  \n",
      "4    Southampton  \n",
      "..           ...  \n",
      "886  Southampton  \n",
      "887  Southampton  \n",
      "888  Southampton  \n",
      "889    Cherbourg  \n",
      "890   Queenstown  \n",
      "\n",
      "[891 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Load the titanic dataset\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "print(\"Titanic Data\")\n",
    "\n",
    "\n",
    "print(titanic_data.columns) # titanic data set\n",
    "\n",
    "print(titanic_data[['survived','pclass', 'sex', 'age', 'sibsp', 'parch', 'class', 'fare', 'embark_town']]) # look at selected columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use Pandas to clean the data.  Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format.  This is called 'Training' or 'Cleaning' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     survived  pclass  sex   age  sibsp  parch      fare  alone  embarked_C  \\\n",
      "0           0       3    1  22.0      1      0    7.2500      0         0.0   \n",
      "1           1       1    0  38.0      1      0   71.2833      0         1.0   \n",
      "2           1       3    0  26.0      0      0    7.9250      1         0.0   \n",
      "3           1       1    0  35.0      1      0   53.1000      0         0.0   \n",
      "4           0       3    1  35.0      0      0    8.0500      1         0.0   \n",
      "..        ...     ...  ...   ...    ...    ...       ...    ...         ...   \n",
      "705         0       2    1  39.0      0      0   26.0000      1         0.0   \n",
      "706         1       2    0  45.0      0      0   13.5000      1         0.0   \n",
      "707         1       1    1  42.0      0      0   26.2875      1         0.0   \n",
      "708         1       1    0  22.0      0      0  151.5500      1         0.0   \n",
      "710         1       1    0  24.0      0      0   49.5042      1         1.0   \n",
      "\n",
      "     embarked_Q  embarked_S  \n",
      "0           0.0         1.0  \n",
      "1           0.0         0.0  \n",
      "2           0.0         1.0  \n",
      "3           0.0         1.0  \n",
      "4           0.0         1.0  \n",
      "..          ...         ...  \n",
      "705         0.0         1.0  \n",
      "706         0.0         1.0  \n",
      "707         1.0         0.0  \n",
      "708         0.0         1.0  \n",
      "710         0.0         0.0  \n",
      "\n",
      "[564 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess the data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "td = titanic_data\n",
    "td.drop(['alive', 'who', 'adult_male', 'class', 'embark_town', 'deck'], axis=1, inplace=True)\n",
    "td.dropna(inplace=True)\n",
    "td['sex'] = td['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "td['alone'] = td['alone'].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "# Encode categorical variables\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(td[['embarked']])\n",
    "onehot = enc.transform(td[['embarked']]).toarray()\n",
    "cols = ['embarked_' + val for val in enc.categories_[0]]\n",
    "td[cols] = pd.DataFrame(onehot)\n",
    "td.drop(['embarked'], axis=1, inplace=True)\n",
    "td.dropna(inplace=True)\n",
    "\n",
    "print(td)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The result of 'Training' data is making it easier to analyze or make conclusions.  In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival.\n",
    "\n",
    "This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person's chances of survival, and using that information to make predictions about whether an individual would have survived or not. \n",
    "\n",
    "- Data description:\n",
    "    - Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file.\n",
    "    - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "    - Name - Name\n",
    "    - Sex - Sex\n",
    "    - Age - Age\n",
    "    - Sibsp - Number of Siblings/Spouses Aboard\n",
    "    - Parch - Number of Parents/Children Aboard\n",
    "    - Ticket - Ticket Number\n",
    "    - Fare - Passenger Fare\n",
    "    - Cabin - Cabin\n",
    "    - Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "- Perished Mean/Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived       0.000000\n",
      "pclass         2.464072\n",
      "sex            0.844311\n",
      "age           31.073353\n",
      "sibsp          0.562874\n",
      "parch          0.398204\n",
      "fare          24.835902\n",
      "alone          0.616766\n",
      "embarked_C     0.185629\n",
      "embarked_Q     0.038922\n",
      "embarked_S     0.775449\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(titanic_data.query(\"survived == 0\").mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Survived Mean/Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived       1.000000\n",
      "pclass         1.878261\n",
      "sex            0.326087\n",
      "age           28.481522\n",
      "sibsp          0.504348\n",
      "parch          0.508696\n",
      "fare          50.188806\n",
      "alone          0.456522\n",
      "embarked_C     0.152174\n",
      "embarked_Q     0.034783\n",
      "embarked_S     0.813043\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(td.query(\"survived == 1\").mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Survived Max and Min Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived        1.0000\n",
      "pclass          3.0000\n",
      "sex             1.0000\n",
      "age            80.0000\n",
      "sibsp           4.0000\n",
      "parch           5.0000\n",
      "fare          512.3292\n",
      "alone           1.0000\n",
      "embarked_C      1.0000\n",
      "embarked_Q      1.0000\n",
      "embarked_S      1.0000\n",
      "dtype: float64\n",
      "survived      1.00\n",
      "pclass        1.00\n",
      "sex           0.00\n",
      "age           0.75\n",
      "sibsp         0.00\n",
      "parch         0.00\n",
      "fare          0.00\n",
      "alone         0.00\n",
      "embarked_C    0.00\n",
      "embarked_Q    0.00\n",
      "embarked_S    0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(td.query(\"survived == 1\").max())\n",
    "print(td.query(\"survived == 1\").min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning <a href=\"https://www.tutorialspoint.com/scikit_learn/scikit_learn_introduction.htm#:~:text=Scikit%2Dlearn%20(Sklearn)%20is,a%20consistence%20interface%20in%20Python\">Visit Tutorials Point</a>\n",
    "> Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.\n",
    "\n",
    "- Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn't seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others.\n",
    "\n",
    "- Code Below prepares data for further analysis and provides an Accuracy.  IMO, you would insert a new passenger and predict survival.  Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down.\n",
    "    - [Decision Trees](https://scikit-learn.org/stable/modules/tree.html#tree), prediction by a piecewise constant approximation.\n",
    "    \n",
    "    - [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), the probabilities describing the possible outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier Accuracy: 0.7705882352941177\n",
      "LogisticRegression Accuracy: 0.788235294117647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnmortensen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets.\n",
    "X = td.drop('survived', axis=1)\n",
    "y = td['survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('DecisionTreeClassifier Accuracy:', accuracy)\n",
    "\n",
    "# Train a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('LogisticRegression Accuracy:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Year      Team  Game  Win  Home  MP  FG  FGA   FGP  TP  TPA   TPP  FT  FTA   FTP  ORB  DRB  TRB  AST  STL  BLK  TOV  PF  PTS\n",
      " 1980    Lakers     1    1     1 240  48   89 0.539   0    0   NaN  13   15 0.867   12   31   43   30    5    9   17  24  109\n",
      " 1980    Lakers     2    0     1 240  48   95 0.505   0    1 0.000   8   12 0.667   15   37   52   32   12    7   26  27  104\n",
      " 1980    Lakers     3    1     0 240  44   92 0.478   0    1 0.000  23   30 0.767   22   34   56   20    5    5   20  25  111\n",
      " 1980    Lakers     4    0     0 240  44   93 0.473   0    0   NaN  14   19 0.737   18   31   49   23   12    6   19  22  102\n",
      " 1980    Lakers     5    1     1 240  41   91 0.451   0    0   NaN  26   33 0.788   19   37   56   28    7    6   21  27  108\n",
      " 1980    Lakers     6    1     0 240  45   92 0.489   0    2 0.000  33   35 0.943   17   35   52   27   14    4   17  22  123\n",
      " 1981   Celtics     1    1     1 240  41   95 0.432   0    1 0.000  16   20 0.800   25   29   54   23    6    5   19  21   98\n",
      " 1981   Celtics     2    0     1 240  41   82 0.500   0    3 0.000   8   13 0.615   14   34   48   17    6    7   22  27   90\n",
      " 1981   Celtics     3    1     0 240  40   89 0.449   2    3 0.667  12   19 0.632   16   28   44   24   12    6   11  25   94\n",
      " 1981   Celtics     4    0     0 240  35   74 0.473   0    3 0.000  16   24 0.667   17   30   47   22    5    6   22  22   86\n",
      " 1981   Celtics     5    1     1 240  41   94 0.436   0    3 0.000  27   35 0.771   19   35   54   25    5    8   14  23  109\n",
      " 1981   Celtics     6    1     0 240  43   78 0.551   1    4 0.250  15   18 0.833    9   28   37   26    6    0   13  21  102\n",
      " 1982    Lakers     1    1     0 240  49   93 0.527   0    0   NaN  26   35 0.743   19   31   50   34   11    7   22  26  124\n",
      " 1982    Lakers     2    0     0 240  35   83 0.422   0    5 0.000  24   37 0.649   17   22   39   25   11    6   18  21   94\n",
      " 1982    Lakers     3    1     1 240  50   91 0.549   1    1 1.000  28   47 0.596   17   31   48   30   15    5   18  30  129\n",
      " 1982    Lakers     4    1     1 240  45   97 0.464   0    1 0.000  21   29 0.724   16   33   49   35   10    4   12  21  111\n",
      " 1982    Lakers     5    0     0 240  47  100 0.470   0    2 0.000   8   16 0.500   26   23   49   31    5    9   24  29  102\n",
      " 1982    Lakers     6    1     1 240  47   87 0.540   0    0   NaN  20   25 0.800   15   34   49   33   12   11   22  26  114\n",
      " 1983    Sixers     1    1     1 240  45   96 0.469   0    0   NaN  23   31 0.742   21   32   53   26   11   13   14  22  113\n",
      " 1983    Sixers     2    1     1 240  40   83 0.482   0    1 0.000  23   32 0.719   16   28   44   22   10    6   18  16  103\n",
      " 1983    Sixers     3    1     0 240  45   94 0.479   0    1 0.000  21   35 0.600   22   32   54   27   14    2   19  23  111\n",
      " 1983    Sixers     4    1     0 240  42   81 0.519   0    1 0.000  31   40 0.775   13   28   41   29    9   11   17  28  115\n",
      " 1984   Celtics     1    1     1 240  43   83 0.518   1    2 0.500  28   32 0.875   12   30   42   25   10    4   16  30  115\n",
      " 1984   Celtics     2    0     1 265  45   98 0.459   0    2 0.000  34   48 0.708   20   30   50   20   13    4   17  27  124\n",
      " 1984   Celtics     3    0     0 240  40  101 0.396   2    5 0.400  22   31 0.710   17   27   44   23    5    5   13  33  104\n",
      " 1984   Celtics     4    1     0 265  48  111 0.432   2    4 0.500  31   37 0.838   27   25   52   30   15    3   14  28  129\n",
      " 1984   Celtics     5    1     1 240  45   87 0.517   3    5 0.600  28   36 0.778   13   38   51   28    8    5   18  32  121\n",
      " 1984   Celtics     6    0     0 240  39   84 0.464   1    3 0.333  29   35 0.829   13   28   41   25    7    8   19  24  108\n",
      " 1984   Celtics     7    1     1 240  34   86 0.395   0    2 0.000  43   51 0.843   20   32   52   18   11    3   14  23  111\n",
      " 1985    Lakers     1    0     0 240  49  100 0.490   2    5 0.400  14   21 0.667    8   27   35   28    9    4   12  23  114\n",
      " 1985    Lakers     2    1     0 240  44   92 0.478   1    2 0.500  20   29 0.690   14   35   49   31    9    5   16  32  109\n",
      " 1985    Lakers     3    1     1 240  52   96 0.542   4    8 0.500  28   36 0.778   13   36   49   34   12    6   12  28  136\n",
      " 1985    Lakers     4    0     1 240  40   83 0.482   1    4 0.250  24   29 0.828    9   31   40   32    7    0   14  24  105\n",
      " 1985    Lakers     5    1     1 240  50   88 0.568   0    2 0.000  20   27 0.741   10   29   39   40    8    7    9  19  120\n",
      " 1985    Lakers     6    1     0 240  43   84 0.512   0    2 0.000  25   36 0.694   12   32   44   27   10    2   12  26  111\n",
      " 1986   Celtics     1    1     1 240  47   84 0.560   1    4 0.250  17   27 0.630   10   32   42   35   15    5   17  17  112\n",
      " 1986   Celtics     2    1     1 240  45   90 0.500   4    9 0.444  23   24 0.958   14   29   43   28    9    8   12  20  117\n",
      " 1986   Celtics     3    0     0 240  42   96 0.438   1    5 0.200  19   21 0.905   22   31   53   28   12    4   19  27  104\n",
      " 1986   Celtics     4    1     0 240  45   78 0.577   2    6 0.333  14   17 0.824   11   29   40   23    8    6   15  17  106\n",
      " 1986   Celtics     5    0     0 240  34   84 0.405   1    6 0.167  27   31 0.871   10   27   37   20    6    7   18  22   96\n",
      " 1986   Celtics     6    1     1 240  49  101 0.485   1    2 0.500  12   18 0.667   23   33   56   34    9   13   14  23  111\n",
      " 1987    Lakers     1    1     1 240  55   99 0.556   1    5 0.200  15   19 0.789   17   30   47   32   10    6   13  24  126\n",
      " 1987    Lakers     2    1     1 240  56   91 0.615   6    8 0.750  23   32 0.719    7   26   33   44    8    8   10  25  141\n",
      " 1987    Lakers     3    0     0 240  40   81 0.494   3   11 0.273  20   27 0.741    8   24   32   18    8    4   10  24  103\n",
      " 1987    Lakers     4    1     0 240  41   85 0.482   2    4 0.500  23   32 0.719   15   31   46   17    4    4   11  16  107\n",
      " 1987    Lakers     5    0     0 240  43   95 0.453   4    8 0.500  18   22 0.818   17   23   40   16    7    4   12  26  108\n",
      " 1987    Lakers     6    1     1 240  45   93 0.484   0    5 0.000  16   23 0.696   13   31   44   33   11    9   11  28  106\n",
      " 1988    Lakers     1    0     1 240  33   83 0.398   2    8 0.250  25   27 0.926   13   22   35   21    5    0   10  18   93\n",
      " 1988    Lakers     2    1     1 240  35   77 0.455   3    6 0.500  35   46 0.761    8   35   43   27    6    5    9  25  108\n",
      " 1988    Lakers     3    1     0 240  37   72 0.514   1    3 0.333  24   34 0.706    5   33   38   21    8    6   11  13   99\n",
      " 1988    Lakers     4    0     0 240  29   72 0.403   0    7 0.000  28   37 0.757    8   26   34   11    3    3   16  30   86\n",
      " 1988    Lakers     5    0     0 240  37   78 0.474   1    4 0.250  19   33 0.576   15   16   31   26    7    3   14  27   94\n",
      " 1988    Lakers     6    1     1 240  34   72 0.472   0    2 0.000  35   43 0.814   13   28   41   30    6    3   14  19  103\n",
      " 1988    Lakers     7    1     1 240  43   77 0.558   3   10 0.300  19   27 0.704   10   31   41   30    6    1   18  22  108\n",
      " 1989   Pistons     1    1     1 240  46   83 0.554   1    2 0.500  16   21 0.762    9   36   45   26    2    5   13  28  109\n",
      " 1989   Pistons     2    1     1 240  44   81 0.543   1    8 0.125  19   21 0.905   13   22   35   21    3    9   10  31  108\n",
      " 1989   Pistons     3    1     0 240  43   83 0.518   1    7 0.143  27   32 0.844   14   30   44   29    8    3   13  30  114\n",
      " 1989   Pistons     4    1     0 240  34   70 0.486   2    3 0.667  35   51 0.686    9   27   36   21    3    4    9  26  105\n",
      " 1990   Pistons     1    1     1 240  34   91 0.374   3   10 0.300  34   46 0.739   19   35   54   16    6    5   13  25  105\n",
      " 1990   Pistons     2    0     1 265  41   90 0.456   8   14 0.571  15   23 0.652   15   29   44   25    2    7   18  28  105\n",
      " 1990   Pistons     3    1     0 240  43   81 0.531   3   10 0.300  32   41 0.780   10   26   36   19    3    4   14  26  121\n",
      " 1990   Pistons     4    1     0 240  42   91 0.462   7   16 0.438  21   24 0.875   12   27   39   19    8    3   12  30  112\n",
      " 1990   Pistons     5    1     0 240  37   81 0.457   4    6 0.667  14   23 0.609   16   28   44   15    9    6   18  29   92\n",
      " 1991     Bulls     1    0     1 240  38   80 0.475   1    7 0.143  14   18 0.778    9   30   39   26   11    5   10  21   91\n",
      " 1991     Bulls     2    1     1 240  50   81 0.617   0    5 0.000   7    8 0.875   11   25   36   35   10    1   14  20  107\n",
      " 1991     Bulls     3    1     0 265  41   85 0.482   1    3 0.333  21   24 0.875   16   30   46   23   10    5   16  21  104\n",
      " 1991     Bulls     4    1     0 240  42   80 0.525   1    3 0.333  12   14 0.857    9   29   38   27    4    8    5  22   97\n",
      " 1991     Bulls     5    1     0 240  42   78 0.538   2    3 0.667  22   28 0.786   10   27   37   28   14    6   18  23  108\n",
      " 1992     Bulls     1    1     1 240  50   91 0.549   7   15 0.467  15   22 0.682   16   28   44   38    7    7   11  21  122\n",
      " 1992     Bulls     2    0     1 265  41   86 0.477   5   15 0.333  17   26 0.654   10   31   41   34    5    7   15  29  104\n",
      " 1992     Bulls     3    1     0 240  37   78 0.474   1    4 0.250  19   29 0.655    9   33   42   25    9    5   16  26   94\n",
      " 1992     Bulls     4    0     0 240  35   77 0.455   4   12 0.333  14   20 0.700    7   26   33   18    7    1   16  27   88\n",
      " 1992     Bulls     5    1     0 240  40   73 0.548   2    6 0.333  37   45 0.822   10   24   34   26    6    6   14  33  119\n",
      " 1992     Bulls     6    1     1 240  38   73 0.521   6   13 0.462  15   21 0.714    9   22   31   20    8    5   18  19   97\n",
      " 1993     Bulls     1    1     0 240  43   81 0.531   3    8 0.375  11   19 0.579   10   31   41   28   10    8   12  17  100\n",
      " 1993     Bulls     2    1     0 240  45   89 0.506   3    7 0.429  18   25 0.720   17   26   43   31   10    7   16  21  111\n",
      " 1993     Bulls     3    0     1 315  55  130 0.423   5   13 0.385   6    9 0.667   27   32   59   32    8    6   16  29  121\n",
      " 1993     Bulls     4    1     1 240  44   83 0.530   3    9 0.333  20   33 0.606   12   30   42   26    8    5   10  26  111\n",
      " 1993     Bulls     5    0     1 240  37   79 0.468   8   18 0.444  16   24 0.667    8   27   35   20    6    4    9  22   98\n",
      " 1993     Bulls     6    1     0 240  39   82 0.476  10   14 0.714  11   20 0.550   12   27   39   24    6    2    8  26   99\n",
      " 1994   Rockets     1    1     1 240  31   74 0.419   4   16 0.250  19   30 0.633   12   37   49   21   10    4   17  21   85\n",
      " 1994   Rockets     2    0     1 240  32   82 0.390   6   22 0.273  13   18 0.722   17   21   38   20   11    5   16  18   83\n",
      " 1994   Rockets     3    1     0 240  30   75 0.400   6   17 0.353  27   31 0.871   14   33   47   19    4    7   17  18   93\n",
      " 1994   Rockets     4    0     0 240  31   72 0.431   6   20 0.300  14   18 0.778    7   26   33   19    5    5   17  27   82\n",
      " 1994   Rockets     5    0     0 240  33   81 0.407   6   18 0.333  12   16 0.750   15   27   42   24    7    4   18  21   84\n",
      " 1994   Rockets     6    1     1 240  32   66 0.485   5   17 0.294  17   23 0.739    6   32   38   21    5    6   14  21   86\n",
      " 1994   Rockets     7    1     1 240  34   73 0.466   4   11 0.364  18   23 0.783    3   30   33   22    8    5    9  23   90\n",
      " 1995   Rockets     1    1     0 265  44   96 0.458  14   32 0.438  18   23 0.783    9   32   41   31    9   10   11  21  120\n",
      " 1995   Rockets     2    1     0 240  39   75 0.520   5   14 0.357  34   41 0.829    9   29   38   18   13    7    9  23  117\n",
      " 1995   Rockets     3    1     1 240  39   84 0.464   7   19 0.368  21   30 0.700   11   35   46   28    7    2    9  21  106\n",
      " 1995   Rockets     4    1     1 240  40   88 0.455  11   27 0.407  22   29 0.759   15   33   48   28   11    1   10  18  113\n",
      " 1996     Bulls     1    1     1 240  37   86 0.430   7   26 0.269  26   31 0.839   17   23   40   20    9    9    7  27  107\n",
      " 1996     Bulls     2    1     1 240  30   77 0.390   4   21 0.190  28   42 0.667   20   25   45   22   10    5   15  27   92\n",
      " 1996     Bulls     3    1     0 240  38   76 0.500   7   15 0.467  25   34 0.735   10   23   33   28    9    2    9  29  108\n",
      " 1996     Bulls     4    0     0 240  32   80 0.400   6   24 0.250  16   19 0.842   15   21   36   22    5    3   17  24   86\n",
      " 1996     Bulls     5    0     0 240  29   77 0.377   3   26 0.115  17   23 0.739   15   25   40   18    3    2   13  27   78\n",
      " 1996     Bulls     6    1     0 240  31   78 0.397   9   25 0.360  16   20 0.800   24   27   51   20   14    4   19  15   87\n",
      " 1997     Bulls     1    1     1 240  34   76 0.447   6   16 0.375  10   15 0.667   10   26   36   23    9    7   14  17   84\n",
      " 1997     Bulls     2    1     1 240  32   69 0.464   6   16 0.375  27   33 0.818   11   30   41   26    8    4   13  26   97\n",
      " 1997     Bulls     3    0     0 240  33   75 0.440  12   32 0.375  15   21 0.714    8   27   35   20    7    8   12  25   93\n",
      " 1997     Bulls     4    0     0 240  32   76 0.421   4   19 0.211   5   12 0.417   10   28   38   17    7    4    8  19   73\n",
      " 1997     Bulls     5    1     0 240  32   72 0.444   6   15 0.400  20   30 0.667   10   32   42   17    8    5    8  25   90\n",
      " 1997     Bulls     6    1     1 240  31   81 0.383   5   14 0.357  23   30 0.767   15   35   50   15    7    4   13  28   90\n",
      " 1998     Bulls     1    0     0 265  34   82 0.415   3   16 0.188  14   16 0.875    9   32   41   14    8    8   14  19   85\n",
      " 1998     Bulls     2    1     0 240  34   80 0.425   3   16 0.188  22   27 0.815   18   18   36   17   10    3    7  21   93\n",
      " 1998     Bulls     3    1     1 240  37   76 0.487   4   11 0.364  18   26 0.692   11   39   50   25   13    5   17  19   96\n",
      " 1998     Bulls     4    1     1 240  27   73 0.370   5   15 0.333  27   40 0.675   17   27   44   18    7    2    9  19   86\n",
      " 1998     Bulls     5    0     1 240  29   75 0.387   7   20 0.350  16   20 0.800   13   20   33   19   11    5   16  25   81\n",
      " 1998     Bulls     6    1     0 240  34   67 0.507   4   10 0.400  15   19 0.789    5   17   22   17   11    4    9  23   87\n",
      " 1999     Spurs     1    1     1 240  32   67 0.478   6   15 0.400  19   31 0.613    4   33   37   25    9    6   14  18   89\n",
      " 1999     Spurs     2    1     1 240  27   63 0.429   2   12 0.167  24   35 0.686    5   36   41   18    5    9   12  13   80\n",
      " 1999     Spurs     3    0     0 240  29   65 0.446   5   13 0.385  18   22 0.818   10   26   36   15    6    3   20  27   81\n",
      " 1999     Spurs     4    1     0 240  35   75 0.467   3    9 0.333  23   28 0.821   14   35   49   23    5    9   17  25   96\n",
      " 1999     Spurs     5    1     0 240  27   67 0.403   5   13 0.385  19   25 0.760    9   31   40   20    7    2   14  17   78\n",
      " 2000    Lakers     1    1     1 240  45   88 0.511   3   12 0.250  11   19 0.579   14   34   48   25    9    6   11  19  104\n",
      " 2000    Lakers     2    1     1 240  36   75 0.480   7   15 0.467  32   57 0.561   11   36   47   29    3    8    9  26  111\n",
      " 2000    Lakers     3    0     0 240  38   76 0.500   7   17 0.412   8   19 0.421    8   25   33   23    9    4   16  27   91\n",
      " 2000    Lakers     4    1     0 265  48   93 0.516   4   12 0.333  20   30 0.667   14   28   42   20    6    4   12  31  120\n",
      " 2000    Lakers     5    0     0 240  36   90 0.400   4   19 0.211  11   21 0.524   13   21   34   18    9    2   12  33   87\n",
      " 2000    Lakers     6    1     1 240  43   90 0.478  10   17 0.588  20   33 0.606   13   31   44   25    5    7    5  24  116\n",
      " 2001    Lakers     1    0     1 265  40   90 0.444   6   13 0.462  15   27 0.556   16   28   44   23   14    9   19  22  101\n",
      " 2001    Lakers     2    1     1 240  38   81 0.469   4   16 0.250  18   26 0.692   15   37   52   29    7   13   16  24   98\n",
      " 2001    Lakers     3    1     0 240  35   75 0.467   4   10 0.400  22   25 0.880   10   30   40   18    8    6   13  26   96\n",
      " 2001    Lakers     4    1     0 240  36   72 0.500  10   19 0.526  18   32 0.563   12   31   43   24    6    6   14  22  100\n",
      " 2001    Lakers     5    1     0 240  32   71 0.451  12   17 0.706  32   45 0.711   14   33   47   21    6   10   12  22  108\n",
      " 2002    Lakers     1    1     1 240  33   72 0.458   1   10 0.100  32   45 0.711   17   33   50   21    8    8   16  20   99\n",
      " 2002    Lakers     2    1     1 240  39   78 0.500   9   16 0.563  19   24 0.792   12   35   47   26    6    7   16  21  106\n",
      " 2002    Lakers     3    1     0 240  37   68 0.544   8   16 0.500  24   35 0.686    7   33   40   17    7   10   19  22  106\n",
      " 2002    Lakers     4    1     0 240  37   71 0.521  11   19 0.579  28   37 0.757    7   32   39   28    7    3    9  15  113\n",
      " 2003     Spurs     1    1     1 240  39   79 0.494   4   10 0.400  19   28 0.679   10   37   47   24    7   12   12  20  101\n",
      " 2003     Spurs     2    0     1 240  33   68 0.485   5   13 0.385  14   25 0.560   14   29   43   17    6    7   21  16   85\n",
      " 2003     Spurs     3    1     0 240  28   67 0.418   5   10 0.500  23   35 0.657    9   29   38   19   10    8   17  17   84\n",
      " 2003     Spurs     4    0     0 240  26   90 0.289   4   18 0.222  20   24 0.833   22   31   53   14   10   10   11  27   76\n",
      " 2003     Spurs     5    1     0 240  33   69 0.478   3    9 0.333  24   29 0.828    8   31   39   15   10    9   15  29   93\n",
      " 2003     Spurs     6    1     1 240  34   74 0.459   3   15 0.200  17   25 0.680   11   44   55   20    4   13   19  14   88\n",
      " 2004   Pistons     1    1     0 240  30   65 0.462   6   12 0.500  21   30 0.700    9   27   36   19    9    4   14  17   87\n",
      " 2004   Pistons     2    0     0 265  32   81 0.395   6   12 0.500  21   31 0.677   19   27   46   19    6    6   14  23   91\n",
      " 2004   Pistons     3    1     1 240  31   76 0.408   5   15 0.333  21   30 0.700   15   36   51   17   11    4   11  16   88\n",
      " 2004   Pistons     4    1     1 240  29   68 0.426   2   13 0.154  28   41 0.683    9   36   45   16    5    4   12  20   88\n",
      " 2004   Pistons     5    1     1 240  35   76 0.461   2   14 0.143  28   39 0.718   20   30   50   18    8    2   12  27  100\n",
      " 2005     Spurs     1    1     1 240  34   79 0.430   4   13 0.308  12   15 0.800   15   34   49   12    3    8   15  17   84\n",
      " 2005     Spurs     2    1     1 240  29   62 0.468  11   24 0.458  28   34 0.824    9   27   36   23   11    7   15  19   97\n",
      " 2005     Spurs     3    0     0 240  29   67 0.433   8   17 0.471  13   20 0.650   10   27   37   16    7    3   18  21   79\n",
      " 2005     Spurs     4    0     0 240  26   70 0.371   5   15 0.333  14   24 0.583   12   32   44   15    1    9   17  21   71\n",
      " 2005     Spurs     5    1     0 265  38   82 0.463   8   20 0.400  12   21 0.571   19   26   45   20    3    3   16  26   96\n",
      " 2005     Spurs     6    0     1 240  31   75 0.413   8   28 0.286  16   26 0.615   13   30   43   15    3    2   11  18   86\n",
      " 2005     Spurs     7    1     1 240  29   68 0.426   7   11 0.636  16   19 0.842    8   30   38   14    4    7   13  20   81\n",
      " 2006      Heat     1    0     0 240  34   78 0.436   5   20 0.250   7   19 0.368   12   33   45   20   10    1   15  25   80\n",
      " 2006      Heat     2    0     0 240  29   70 0.414   7   17 0.412  20   32 0.625    8   24   32   16    5    3   13  23   85\n",
      " 2006      Heat     3    1     1 240  37   76 0.487   4   14 0.286  20   34 0.588   16   33   49   13    9    2   20  23   98\n",
      " 2006      Heat     4    1     1 240  34   66 0.515   7   19 0.368  23   36 0.639    6   42   48   19    7    7   18  26   98\n",
      " 2006      Heat     5    1     1 265  31   69 0.449   7   17 0.412  32   49 0.653    7   26   33   14    7    0   11  26  101\n",
      " 2006      Heat     6    1     0 240  35   78 0.449   2   18 0.111  23   37 0.622   14   42   56   18    9   10   19  23   95\n",
      " 2007     Spurs     1    1     1 240  34   75 0.453   6   16 0.375  11   16 0.688   13   30   43   18    6    5   14  15   85\n",
      " 2007     Spurs     2    1     1 240  37   77 0.481   8   24 0.333  21   26 0.808   12   34   46   21    6    6   10  23  103\n",
      " 2007     Spurs     3    1     0 240  28   68 0.412  10   19 0.526   9   16 0.563    7   34   41   15    7    2   14  15   75\n",
      " 2007     Spurs     4    1     0 240  29   68 0.426   5   19 0.263  20   34 0.588   11   34   45   13    5    2   14  19   83\n",
      " 2008   Celtics     1    1     1 240  32   76 0.421   6   19 0.316  28   35 0.800   10   36   46   20    6    3   13  22   98\n",
      " 2008   Celtics     2    1     1 240  36   68 0.529   9   14 0.643  27   38 0.711   10   27   37   31    7    3   15  21  108\n",
      " 2008   Celtics     3    0     0 240  29   83 0.349   8   18 0.444  15   22 0.682   14   31   45   19    8    8   13  28   81\n",
      " 2008   Celtics     4    1     0 240  33   73 0.452   8   22 0.364  23   28 0.821    6   34   40   15    7    4   11  24   97\n",
      " 2008   Celtics     5    0     0 240  33   77 0.429   8   22 0.364  24   31 0.774   11   26   37   20    6    2   18  28   98\n",
      " 2008   Celtics     6    1     1 240  43   87 0.494  13   26 0.500  32   37 0.865   14   34   48   33   18    4    7  25  131\n",
      " 2009    Lakers     1    1     1 240  41   89 0.461   3    9 0.333  15   18 0.833   15   40   55   18    4    7    8  23  100\n",
      " 2009    Lakers     2    1     1 265  36   78 0.462   5   15 0.333  24   28 0.857    4   31   35   20   12    6   12  25  101\n",
      " 2009    Lakers     3    0     0 240  40   78 0.513   8   23 0.348  16   26 0.615   11   16   27   16    6    3   13  25  104\n",
      " 2009    Lakers     4    1     0 265  38   91 0.418   8   23 0.348  15   20 0.750   10   29   39   15    8    2    7  28   99\n",
      " 2009    Lakers     5    1     0 240  35   80 0.438   8   16 0.500  21   28 0.750   13   34   47   13    6    8   10  20   99\n",
      " 2010    Lakers     1    1     1 240  37   76 0.487   4   10 0.400  24   31 0.774   12   30   42   18    6    7   12  26  102\n",
      " 2010    Lakers     2    0     1 240  29   71 0.408   5   22 0.227  31   41 0.756   10   29   39   18    8   14   15  29   94\n",
      " 2010    Lakers     3    1     0 240  34   76 0.447   2   15 0.133  21   24 0.875   11   32   43   13    2    7    8  20   91\n",
      " 2010    Lakers     4    0     0 240  32   71 0.451   7   20 0.350  18   22 0.818    8   26   34   13    6    3   15  23   89\n",
      " 2010    Lakers     5    0     0 240  31   78 0.397   7   19 0.368  17   26 0.654   16   18   34   12    9    1   13  22   86\n",
      " 2010    Lakers     6    1     1 240  33   79 0.418   6   19 0.316  17   19 0.895   12   40   52   17   13    8   13  17   89\n",
      " 2010    Lakers     7    1     1 240  27   83 0.325   4   20 0.200  25   37 0.676   23   30   53   11    7    3   11  19   83\n",
      " 2011 Mavericks     1    0     0 240  25   67 0.373   9   22 0.409  25   32 0.781    6   30   36   18    6    8   11  22   84\n",
      " 2011 Mavericks     2    1     0 240  36   75 0.480   6   17 0.353  17   21 0.810   11   30   41   18    8    2   18  20   95\n",
      " 2011 Mavericks     3    0     1 240  28   70 0.400   8   21 0.381  22   27 0.815   12   30   42   18    3    8   14  14   86\n",
      " 2011 Mavericks     4    1     1 240  29   73 0.397   4   19 0.211  24   30 0.800   12   29   41   13    7    2   11  18   86\n",
      " 2011 Mavericks     5    1     1 240  39   69 0.565  13   19 0.684  21   27 0.778    4   22   26   23    8    3   11  20  112\n",
      " 2011 Mavericks     6    1     0 240  41   82 0.500  11   26 0.423  12   18 0.667   10   30   40   19   11    1   14  24  105\n",
      " 2012      Heat     1    0     0 240  36   78 0.462   8   19 0.421  14   18 0.778    7   28   35   20    6    1   10  19   94\n",
      " 2012      Heat     2    1     0 240  36   76 0.474   6   14 0.429  22   25 0.880   11   29   40   13    5    4   13  21  100\n",
      " 2012      Heat     3    1     1 240  28   74 0.378   4   13 0.308  31   35 0.886   14   31   45   13    6    5   12  19   91\n",
      " 2012      Heat     4    1     1 240  38   79 0.481  10   26 0.385  18   25 0.720    9   31   40   19    8    2    9  18  104\n",
      " 2012      Heat     5    1     1 240  40   77 0.519  14   26 0.538  27   33 0.818    8   33   41   25    8    7   13  21  121\n",
      " 2012    'Heat'     1    0     1 240  34   78 0.436   8   25 0.320  12   17 0.706    9   37   46   20    4    2    8  12   88\n",
      " 2013      Heat     2    1     1 240  41   83 0.494  10   19 0.526  11   14 0.786    9   27   36   22    9    6    6  17  103\n",
      " 2013      Heat     3    0     0 240  31   76 0.408   8   18 0.444   7   10 0.700    9   27   36   21    9    8   16  21   77\n",
      " 2013      Heat     4    1     0 240  45   85 0.529   4   12 0.333  15   17 0.882    7   34   41   23   13    7    9  26  109\n",
      " 2013      Heat     5    0     0 240  37   86 0.430  11   23 0.478  19   23 0.826   12   22   34   25    8    3   13  24  104\n",
      " 2013      Heat     6    1     1 265  38   81 0.469  11   19 0.579  16   21 0.762   12   30   42   23   10    6   15  26  103\n",
      " 2013      Heat     7    1     1 240  36   82 0.439  12   32 0.375  11   16 0.688   11   32   43   14    8    4   16  19   95\n",
      " 2014     Spurs     1    1     1 240  40   68 0.588  13   25 0.520  17   22 0.773    5   34   39   30    8    4   22  14  110\n",
      " 2014     Spurs     2    0     1 240  36   82 0.439  12   26 0.462  12   20 0.600   11   26   37   26    7    1   11  20   96\n",
      " 2014     Spurs     3    1     0 240  38   64 0.594   9   20 0.450  26   32 0.813    5   24   29   21   12    4   12  25  111\n",
      " 2014     Spurs     4    1     0 240  40   70 0.571   9   21 0.429  18   25 0.720   12   32   44   25    8    4   14  19  107\n",
      " 2014     Spurs     5    1     1 240  37   78 0.474  12   26 0.462  18   23 0.783    6   34   40   25    5    4    8  21  104\n",
      " 2015  Warriors     1    1     1 265  39   88 0.443  10   27 0.370  20   22 0.909   11   37   48   24    8    5   12  16  108\n",
      " 2015  Warriors     2    0     1 265  33   83 0.398   8   35 0.229  19   25 0.760   10   35   45   16   11    7   18  31   93\n",
      " 2015  Warriors     3    0     0 240  36   90 0.400  12   34 0.353   7   12 0.583   18   28   46   21    6    7   14  25   91\n",
      " 2015  Warriors     4    1     0 240  36   77 0.468  12   30 0.400  19   27 0.704    6   38   44   24    5    5    7  21  103\n",
      " 2015  Warriors     5    1     1 240  36   75 0.480  12   26 0.462  20   34 0.588   11   32   43   25    7    2   16  25  104\n",
      " 2015  Warriors     6    1     0 240  37   85 0.435  13   34 0.382  18   29 0.621    7   32   39   28   11    4    9  27  105\n",
      " 2016 Cavaliers     1    0     0 240  32   84 0.381   7   21 0.333  18   20 0.900   15   32   47   17    7    4   15  17   89\n",
      " 2016 Cavaliers     2    0     0 240  28   79 0.354   5   23 0.217  16   24 0.667    9   25   34   15   15    3   17  19   77\n",
      " 2016 Cavaliers     3    1     1 240  48   91 0.527  12   25 0.480  12   17 0.706   17   35   52   23    8    3   13  25  120\n",
      " 2016 Cavaliers     4    0     1 240  38   81 0.469   6   25 0.240  15   26 0.577   16   24   40   15    5    6   11  24   97\n",
      " 2016 Cavaliers     5    1     0 240  44   83 0.530  10   24 0.417  14   23 0.609    8   33   41   15   11    9   16  22  112\n",
      " 2016 Cavaliers     6    1     1 240  40   77 0.519  10   27 0.370  25   32 0.781    8   37   45   24   12    7   10  25  115\n",
      " 2016 Cavaliers     7    1     0 240  33   82 0.402   6   25 0.240  21   25 0.840    9   39   48   17    7    6   11  15   93\n",
      " 2017  Warriors     1    1     1 240  45  106 0.425  12   33 0.364  11   16 0.688   14   36   50   31   12    3    4  24  113\n",
      " 2017  Warriors     2    1     1 240  46   89 0.517  18   43 0.419  22   24 0.917   10   43   53   34    5    7   20  19  132\n",
      " 2017  Warriors     3    1     0 240  40   83 0.482  16   33 0.485  22   24 0.917    8   36   44   29    8    4   18  28  118\n",
      " 2017 Warriorrs     4    0     0 240  39   87 0.448  11   39 0.282  27   36 0.750   16   24   40   26    5    6   12  27  116\n",
      " 2017  Warriors     5    1     1 240  46   90 0.511  14   38 0.368  23   28 0.821   13   29   42   27    8    2   13  24  129\n",
      " 2018  Warriors     1    1     1 265  46   90 0.511  13   36 0.361  19   20 0.950    4   34   38   31   10    6    7  18  124\n",
      " 2018  Warriors     2    1     1 240  47   82 0.573  15   36 0.417  13   21 0.619    7   34   41   28    3    8   12  25  122\n",
      " 2018  Warriors     3    1     0 240  42   81 0.519   9   26 0.346  17   19 0.895    6   31   37   27    6    5   10  20  110\n",
      " 2018  Warriors     4    1     0 240  39   86 0.453  14   38 0.368  16   16 1.000   10   34   44   25    7   13    8  24  108\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('files/championsdata.csv')\n",
    "\n",
    "print((df).to_string(index=False))\n",
    "# What part of the data set needs to be cleaned?\n",
    "# From PBL learning, what is a good time to clean data?  Hint, remember Garbage in, Garbage out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year       Team  Game  Win  Home   MP  FG  FGA    FGP  TP  ...    FTP  \\\n",
      "143  2005      Spurs     4    0     0  240  26   70  0.371   5  ...  0.583   \n",
      "100  1997      Bulls     4    0     0  240  32   76  0.421   4  ...  0.417   \n",
      "155  2007      Spurs     3    1     0  240  28   68  0.412  10  ...  0.563   \n",
      "132  2003      Spurs     4    0     0  240  26   90  0.289   4  ...  0.833   \n",
      "205  2016  Cavaliers     2    0     0  240  28   79  0.354   5  ...  0.667   \n",
      "..    ...        ...   ...  ...   ...  ...  ..  ...    ...  ..  ...    ...   \n",
      "25   1984    Celtics     4    1     0  265  48  111  0.432   2  ...  0.838   \n",
      "162  2008    Celtics     6    1     1  240  43   87  0.494  13  ...  0.865   \n",
      "212  2017   Warriors     2    1     1  240  46   89  0.517  18  ...  0.917   \n",
      "31   1985     Lakers     3    1     1  240  52   96  0.542   4  ...  0.778   \n",
      "42   1987     Lakers     2    1     1  240  56   91  0.615   6  ...  0.719   \n",
      "\n",
      "     ORB  DRB  TRB  AST  STL  BLK  TOV  PF  PTS  \n",
      "143   12   32   44   15    1    9   17  21   71  \n",
      "100   10   28   38   17    7    4    8  19   73  \n",
      "155    7   34   41   15    7    2   14  15   75  \n",
      "132   22   31   53   14   10   10   11  27   76  \n",
      "205    9   25   34   15   15    3   17  19   77  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ..  ...  \n",
      "25    27   25   52   30   15    3   14  28  129  \n",
      "162   14   34   48   33   18    4    7  25  131  \n",
      "212   10   43   53   34    5    7   20  19  132  \n",
      "31    13   36   49   34   12    6   12  28  136  \n",
      "42     7   26   33   44    8    8   10  25  141  \n",
      "\n",
      "[220 rows x 24 columns]\n",
      "\n",
      "     Year      Team  Game  Win  Home   MP  FG  FGA    FGP  TP  ...    FTP  \\\n",
      "42   1987    Lakers     2    1     1  240  56   91  0.615   6  ...  0.719   \n",
      "31   1985    Lakers     3    1     1  240  52   96  0.542   4  ...  0.778   \n",
      "212  2017  Warriors     2    1     1  240  46   89  0.517  18  ...  0.917   \n",
      "162  2008   Celtics     6    1     1  240  43   87  0.494  13  ...  0.865   \n",
      "215  2017  Warriors     5    1     1  240  46   90  0.511  14  ...  0.821   \n",
      "..    ...       ...   ...  ...   ...  ...  ..  ...    ...  ..  ...    ...   \n",
      "188  2013      Heat     3    0     0  240  31   76  0.408   8  ...  0.700   \n",
      "132  2003     Spurs     4    0     0  240  26   90  0.289   4  ...  0.833   \n",
      "155  2007     Spurs     3    1     0  240  28   68  0.412  10  ...  0.563   \n",
      "100  1997     Bulls     4    0     0  240  32   76  0.421   4  ...  0.417   \n",
      "143  2005     Spurs     4    0     0  240  26   70  0.371   5  ...  0.583   \n",
      "\n",
      "     ORB  DRB  TRB  AST  STL  BLK  TOV  PF  PTS  \n",
      "42     7   26   33   44    8    8   10  25  141  \n",
      "31    13   36   49   34   12    6   12  28  136  \n",
      "212   10   43   53   34    5    7   20  19  132  \n",
      "162   14   34   48   33   18    4    7  25  131  \n",
      "215   13   29   42   27    8    2   13  24  129  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ..  ...  \n",
      "188    9   27   36   21    9    8   16  21   77  \n",
      "132   22   31   53   14   10   10   11  27   76  \n",
      "155    7   34   41   15    7    2   14  15   75  \n",
      "100   10   28   38   17    7    4    8  19   73  \n",
      "143   12   32   44   15    1    9   17  21   71  \n",
      "\n",
      "[220 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values(by=['PTS']))\n",
    "\n",
    "print()\n",
    "\n",
    "#sort the values in reverse order\n",
    "print(df.sort_values(by=['PTS'], ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b83f92cf186cc163d5a14417fbfa48051acb04e5b071b5df17e157d3a041b7f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
